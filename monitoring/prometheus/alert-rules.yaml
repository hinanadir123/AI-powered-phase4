# Task: T5.6.4 - Prometheus Alert Rules
# Implements: phase5-spec.md Section 7.4 (Alerting)
# Generated by: cicd-monitoring-agent
# Date: 2026-02-15

apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-alert-rules
  namespace: monitoring
  labels:
    app: prometheus
    component: alerting
data:
  alert-rules.yml: |
    groups:
      # Application Alerts
      - name: application_alerts
        interval: 30s
        rules:
          # High Error Rate Alert
          - alert: HighErrorRate
            expr: |
              (
                sum(rate(http_requests_total{status=~"5.."}[5m])) by (service)
                /
                sum(rate(http_requests_total[5m])) by (service)
              ) * 100 > 5
            for: 5m
            labels:
              severity: critical
              category: application
              service: "{{ $labels.service }}"
            annotations:
              summary: "High error rate detected on {{ $labels.service }}"
              description: "Error rate is {{ $value | humanizePercentage }} (threshold: 5%) for service {{ $labels.service }}"
              runbook: "https://docs.todo-app.com/runbooks/high-error-rate"
              dashboard: "https://grafana.todo-app.com/d/app-dashboard"

          # High Latency Alert (p95 > 1000ms)
          - alert: HighLatency
            expr: |
              histogram_quantile(0.95,
                sum(rate(http_request_duration_seconds_bucket[5m])) by (le, service)
              ) > 1
            for: 5m
            labels:
              severity: warning
              category: application
              service: "{{ $labels.service }}"
            annotations:
              summary: "High latency detected on {{ $labels.service }}"
              description: "P95 latency is {{ $value | humanizeDuration }} (threshold: 1000ms) for service {{ $labels.service }}"
              runbook: "https://docs.todo-app.com/runbooks/high-latency"
              dashboard: "https://grafana.todo-app.com/d/app-dashboard"

          # Service Down Alert
          - alert: ServiceDown
            expr: up{job=~"backend|frontend|reminder-worker"} == 0
            for: 5m
            labels:
              severity: critical
              category: application
              service: "{{ $labels.job }}"
            annotations:
              summary: "Service {{ $labels.job }} is down"
              description: "Service {{ $labels.job }} has been down for more than 5 minutes"
              runbook: "https://docs.todo-app.com/runbooks/service-down"
              dashboard: "https://grafana.todo-app.com/d/app-dashboard"

          # High Request Rate (Potential DDoS)
          - alert: HighRequestRate
            expr: |
              sum(rate(http_requests_total[1m])) by (service) > 1000
            for: 2m
            labels:
              severity: warning
              category: application
              service: "{{ $labels.service }}"
            annotations:
              summary: "Unusually high request rate on {{ $labels.service }}"
              description: "Request rate is {{ $value | humanize }} req/s (threshold: 1000 req/s) for service {{ $labels.service }}"
              runbook: "https://docs.todo-app.com/runbooks/high-request-rate"
              dashboard: "https://grafana.todo-app.com/d/app-dashboard"

      # Infrastructure Alerts
      - name: infrastructure_alerts
        interval: 30s
        rules:
          # High CPU Usage Alert
          - alert: HighCPUUsage
            expr: |
              (
                sum(rate(container_cpu_usage_seconds_total{namespace="todo-app"}[5m])) by (pod)
                /
                sum(container_spec_cpu_quota{namespace="todo-app"} / container_spec_cpu_period{namespace="todo-app"}) by (pod)
              ) * 100 > 80
            for: 10m
            labels:
              severity: warning
              category: infrastructure
              pod: "{{ $labels.pod }}"
            annotations:
              summary: "High CPU usage on pod {{ $labels.pod }}"
              description: "CPU usage is {{ $value | humanizePercentage }} (threshold: 80%) for pod {{ $labels.pod }}"
              runbook: "https://docs.todo-app.com/runbooks/high-cpu"
              dashboard: "https://grafana.todo-app.com/d/infra-dashboard"

          # High Memory Usage Alert
          - alert: HighMemoryUsage
            expr: |
              (
                sum(container_memory_working_set_bytes{namespace="todo-app"}) by (pod)
                /
                sum(container_spec_memory_limit_bytes{namespace="todo-app"}) by (pod)
              ) * 100 > 90
            for: 5m
            labels:
              severity: critical
              category: infrastructure
              pod: "{{ $labels.pod }}"
            annotations:
              summary: "High memory usage on pod {{ $labels.pod }}"
              description: "Memory usage is {{ $value | humanizePercentage }} (threshold: 90%) for pod {{ $labels.pod }}"
              runbook: "https://docs.todo-app.com/runbooks/high-memory"
              dashboard: "https://grafana.todo-app.com/d/infra-dashboard"

          # Low Disk Space Alert
          - alert: LowDiskSpace
            expr: |
              (
                node_filesystem_avail_bytes{mountpoint="/"}
                /
                node_filesystem_size_bytes{mountpoint="/"}
              ) * 100 < 10
            for: 5m
            labels:
              severity: warning
              category: infrastructure
              node: "{{ $labels.instance }}"
            annotations:
              summary: "Low disk space on node {{ $labels.instance }}"
              description: "Available disk space is {{ $value | humanizePercentage }} (threshold: 10%) on node {{ $labels.instance }}"
              runbook: "https://docs.todo-app.com/runbooks/low-disk-space"
              dashboard: "https://grafana.todo-app.com/d/infra-dashboard"

          # Pod Restart Loop Alert
          - alert: PodRestartLoop
            expr: |
              rate(kube_pod_container_status_restarts_total{namespace="todo-app"}[10m]) > 0.3
            for: 10m
            labels:
              severity: critical
              category: infrastructure
              pod: "{{ $labels.pod }}"
            annotations:
              summary: "Pod {{ $labels.pod }} is in restart loop"
              description: "Pod {{ $labels.pod }} has restarted {{ $value | humanize }} times in the last 10 minutes"
              runbook: "https://docs.todo-app.com/runbooks/pod-restart-loop"
              dashboard: "https://grafana.todo-app.com/d/infra-dashboard"

      # Kafka Alerts
      - name: kafka_alerts
        interval: 30s
        rules:
          # High Consumer Lag Alert
          - alert: HighKafkaConsumerLag
            expr: |
              kafka_consumergroup_lag{topic=~"task-events|reminders|task-updates"} > 1000
            for: 5m
            labels:
              severity: warning
              category: kafka
              topic: "{{ $labels.topic }}"
              consumergroup: "{{ $labels.consumergroup }}"
            annotations:
              summary: "High consumer lag on topic {{ $labels.topic }}"
              description: "Consumer lag is {{ $value | humanize }} messages (threshold: 1000) for topic {{ $labels.topic }} and consumer group {{ $labels.consumergroup }}"
              runbook: "https://docs.todo-app.com/runbooks/high-consumer-lag"
              dashboard: "https://grafana.todo-app.com/d/kafka-dashboard"

          # Consumer Group Down Alert
          - alert: KafkaConsumerGroupDown
            expr: |
              kafka_consumergroup_members{topic=~"task-events|reminders|task-updates"} == 0
            for: 5m
            labels:
              severity: critical
              category: kafka
              topic: "{{ $labels.topic }}"
              consumergroup: "{{ $labels.consumergroup }}"
            annotations:
              summary: "Kafka consumer group {{ $labels.consumergroup }} is down"
              description: "No active consumers in group {{ $labels.consumergroup }} for topic {{ $labels.topic }}"
              runbook: "https://docs.todo-app.com/runbooks/consumer-group-down"
              dashboard: "https://grafana.todo-app.com/d/kafka-dashboard"

          # Topic Partition Offline Alert
          - alert: KafkaTopicPartitionOffline
            expr: |
              kafka_topic_partition_in_sync_replica < kafka_topic_partition_replicas
            for: 5m
            labels:
              severity: critical
              category: kafka
              topic: "{{ $labels.topic }}"
              partition: "{{ $labels.partition }}"
            annotations:
              summary: "Kafka partition offline for topic {{ $labels.topic }}"
              description: "Partition {{ $labels.partition }} of topic {{ $labels.topic }} has {{ $value | humanize }} in-sync replicas (expected: {{ $labels.replicas }})"
              runbook: "https://docs.todo-app.com/runbooks/partition-offline"
              dashboard: "https://grafana.todo-app.com/d/kafka-dashboard"

          # Replication Lag Alert
          - alert: KafkaReplicationLag
            expr: |
              kafka_topic_partition_replica_lag > 100
            for: 5m
            labels:
              severity: warning
              category: kafka
              topic: "{{ $labels.topic }}"
              partition: "{{ $labels.partition }}"
            annotations:
              summary: "High replication lag on topic {{ $labels.topic }}"
              description: "Replication lag is {{ $value | humanize }} messages (threshold: 100) for partition {{ $labels.partition }} of topic {{ $labels.topic }}"
              runbook: "https://docs.todo-app.com/runbooks/replication-lag"
              dashboard: "https://grafana.todo-app.com/d/kafka-dashboard"

      # Dapr Alerts
      - name: dapr_alerts
        interval: 30s
        rules:
          # Dapr Sidecar Unhealthy Alert
          - alert: DaprSidecarUnhealthy
            expr: |
              dapr_sidecar_health_status{namespace="todo-app"} == 0
            for: 5m
            labels:
              severity: critical
              category: dapr
              pod: "{{ $labels.pod }}"
            annotations:
              summary: "Dapr sidecar unhealthy on pod {{ $labels.pod }}"
              description: "Dapr sidecar on pod {{ $labels.pod }} has been unhealthy for more than 5 minutes"
              runbook: "https://docs.todo-app.com/runbooks/dapr-sidecar-unhealthy"
              dashboard: "https://grafana.todo-app.com/d/dapr-dashboard"

          # Dapr Component Initialization Failed Alert
          - alert: DaprComponentInitFailed
            expr: |
              dapr_component_loaded{namespace="todo-app"} == 0
            for: 5m
            labels:
              severity: critical
              category: dapr
              component: "{{ $labels.component }}"
            annotations:
              summary: "Dapr component {{ $labels.component }} failed to initialize"
              description: "Dapr component {{ $labels.component }} has failed to load for more than 5 minutes"
              runbook: "https://docs.todo-app.com/runbooks/dapr-component-init-failed"
              dashboard: "https://grafana.todo-app.com/d/dapr-dashboard"

          # High Dapr Sidecar Latency Alert
          - alert: HighDaprSidecarLatency
            expr: |
              histogram_quantile(0.95,
                sum(rate(dapr_http_server_request_duration_seconds_bucket[5m])) by (le, pod)
              ) > 0.5
            for: 5m
            labels:
              severity: warning
              category: dapr
              pod: "{{ $labels.pod }}"
            annotations:
              summary: "High Dapr sidecar latency on pod {{ $labels.pod }}"
              description: "P95 latency is {{ $value | humanizeDuration }} (threshold: 500ms) for Dapr sidecar on pod {{ $labels.pod }}"
              runbook: "https://docs.todo-app.com/runbooks/high-dapr-latency"
              dashboard: "https://grafana.todo-app.com/d/dapr-dashboard"
