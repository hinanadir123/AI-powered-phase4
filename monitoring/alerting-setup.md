# Alerting Setup Guide
## Todo AI Chatbot - Prometheus Alertmanager Configuration

**Task:** T5.6.4 - Setup Alerting Rules for Prometheus
**Implements:** phase5-spec.md Section 7.4 (Alerting)
**Generated by:** cicd-monitoring-agent
**Date:** 2026-02-15

---

## 1. Overview

This guide provides comprehensive instructions for setting up alerting for the Todo AI Chatbot using Prometheus and Alertmanager. The alerting system monitors application health, infrastructure resources, Kafka messaging, and Dapr components.

### Alert Categories

- **Application Alerts**: Error rates, latency, service availability, request rates
- **Infrastructure Alerts**: CPU usage, memory usage, disk space, pod restarts
- **Kafka Alerts**: Consumer lag, consumer group health, partition status, replication lag
- **Dapr Alerts**: Sidecar health, component initialization, sidecar latency

### Alert Severity Levels

- **Critical**: Requires immediate attention (PagerDuty + Slack)
- **Warning**: Requires attention within hours (Slack)
- **Info**: Informational only (Email)

---

## 2. Prerequisites

Before setting up alerting, ensure you have:

- Kubernetes cluster running (Minikube, AKS, or GKE)
- Prometheus deployed and collecting metrics (Task T5.6.2)
- Grafana deployed for visualization (Task T5.6.2)
- kubectl configured to access the cluster
- Helm 3.x installed (optional, for easier deployment)

---

## 3. Configuration Files

The alerting system consists of the following configuration files:

```
monitoring/
├── prometheus/
│   └── alert-rules.yaml              # Prometheus alert rules ConfigMap
├── alertmanager/
│   ├── alertmanager-config.yaml      # Alertmanager configuration
│   ├── alertmanager-deployment.yaml  # Alertmanager deployment
│   ├── alertmanager-service.yaml     # Alertmanager service
│   └── alertmanager-templates.yaml   # Notification templates
└── alerting-setup.md                 # This document
```

---

## 4. Setup Instructions

### Step 1: Create Monitoring Namespace

```bash
kubectl create namespace monitoring
```

### Step 2: Configure Alert Notification Channels

Before deploying Alertmanager, configure your notification channels by creating secrets:

#### Email Configuration (SMTP)

```bash
# Replace with your SMTP credentials
kubectl create secret generic alertmanager-secrets \
  --from-literal=smtp-password='your-smtp-password' \
  -n monitoring
```

#### Slack Configuration

1. Create a Slack webhook URL:
   - Go to https://api.slack.com/apps
   - Create a new app or select existing app
   - Enable Incoming Webhooks
   - Create webhook for desired channels (#alerts-critical, #alerts-warnings, etc.)

2. Store webhook URL in secret:

```bash
kubectl create secret generic alertmanager-secrets \
  --from-literal=slack-webhook-url='https://hooks.slack.com/services/YOUR/WEBHOOK/URL' \
  -n monitoring --dry-run=client -o yaml | kubectl apply -f -
```

#### Discord Configuration (Optional)

```bash
kubectl create secret generic alertmanager-secrets \
  --from-literal=discord-webhook-url='https://discord.com/api/webhooks/YOUR/WEBHOOK/URL' \
  -n monitoring --dry-run=client -o yaml | kubectl apply -f -
```

#### PagerDuty Configuration (Optional)

1. Get PagerDuty integration key from your PagerDuty service
2. Store in secret:

```bash
kubectl create secret generic alertmanager-secrets \
  --from-literal=pagerduty-service-key='your-pagerduty-integration-key' \
  -n monitoring --dry-run=client -o yaml | kubectl apply -f -
```

### Step 3: Deploy Alert Rules to Prometheus

```bash
# Apply Prometheus alert rules
kubectl apply -f monitoring/prometheus/alert-rules.yaml

# Verify ConfigMap is created
kubectl get configmap prometheus-alert-rules -n monitoring

# Update Prometheus to load alert rules
# If using Prometheus Operator, the rules will be auto-loaded
# If using standalone Prometheus, update prometheus.yml to include:
# rule_files:
#   - /etc/prometheus/rules/*.yml
```

### Step 4: Deploy Alertmanager

```bash
# Apply Alertmanager configuration
kubectl apply -f monitoring/alertmanager/alertmanager-config.yaml

# Apply notification templates
kubectl apply -f monitoring/alertmanager/alertmanager-templates.yaml

# Deploy Alertmanager
kubectl apply -f monitoring/alertmanager/alertmanager-deployment.yaml

# Deploy Alertmanager service
kubectl apply -f monitoring/alertmanager/alertmanager-service.yaml

# Verify deployment
kubectl get pods -n monitoring -l app=alertmanager
kubectl get svc -n monitoring -l app=alertmanager
```

### Step 5: Configure Prometheus to Send Alerts to Alertmanager

Update Prometheus configuration to include Alertmanager endpoint:

```yaml
# Add to prometheus.yml
alerting:
  alertmanagers:
    - static_configs:
        - targets:
            - alertmanager:9093
```

If using Prometheus Operator, create an AlertmanagerConfig:

```bash
kubectl apply -f - <<EOF
apiVersion: v1
kind: Service
metadata:
  name: prometheus-operated
  namespace: monitoring
spec:
  ports:
  - name: web
    port: 9090
    targetPort: 9090
  selector:
    app: prometheus
---
apiVersion: monitoring.coreos.com/v1
kind: Alertmanager
metadata:
  name: main
  namespace: monitoring
spec:
  replicas: 2
  serviceAccountName: alertmanager
  configSecret: alertmanager-config
EOF
```

### Step 6: Verify Alerting Setup

```bash
# Check Prometheus targets
kubectl port-forward -n monitoring svc/prometheus 9090:9090
# Open http://localhost:9090/targets

# Check Prometheus alerts
# Open http://localhost:9090/alerts

# Check Alertmanager UI
kubectl port-forward -n monitoring svc/alertmanager 9093:9093
# Open http://localhost:9093

# Check Alertmanager configuration
curl http://localhost:9093/api/v1/status
```

---

## 5. Alert Rules Reference

### Application Alerts

| Alert Name | Threshold | Duration | Severity | Description |
|------------|-----------|----------|----------|-------------|
| HighErrorRate | > 5% | 5 minutes | Critical | Error rate exceeds 5% |
| HighLatency | p95 > 1000ms | 5 minutes | Warning | Response time too high |
| ServiceDown | up == 0 | 5 minutes | Critical | Service is not responding |
| HighRequestRate | > 1000 req/s | 2 minutes | Warning | Potential DDoS attack |

### Infrastructure Alerts

| Alert Name | Threshold | Duration | Severity | Description |
|------------|-----------|----------|----------|-------------|
| HighCPUUsage | > 80% | 10 minutes | Warning | CPU usage is high |
| HighMemoryUsage | > 90% | 5 minutes | Critical | Memory usage is critical |
| LowDiskSpace | < 10% | 5 minutes | Warning | Disk space is low |
| PodRestartLoop | > 3 restarts | 10 minutes | Critical | Pod is restarting frequently |

### Kafka Alerts

| Alert Name | Threshold | Duration | Severity | Description |
|------------|-----------|----------|----------|-------------|
| HighKafkaConsumerLag | > 1000 messages | 5 minutes | Warning | Consumer is lagging |
| KafkaConsumerGroupDown | 0 members | 5 minutes | Critical | No active consumers |
| KafkaTopicPartitionOffline | ISR < replicas | 5 minutes | Critical | Partition is offline |
| KafkaReplicationLag | > 100 messages | 5 minutes | Warning | Replication is lagging |

### Dapr Alerts

| Alert Name | Threshold | Duration | Severity | Description |
|------------|-----------|----------|----------|-------------|
| DaprSidecarUnhealthy | health == 0 | 5 minutes | Critical | Dapr sidecar is unhealthy |
| DaprComponentInitFailed | loaded == 0 | 5 minutes | Critical | Component failed to load |
| HighDaprSidecarLatency | p95 > 500ms | 5 minutes | Warning | Dapr sidecar latency is high |

---

## 6. Testing Alerts

Use the provided test script to verify alerts are working:

```bash
# Make script executable
chmod +x scripts/test-alerts.sh

# Test all alerts
./scripts/test-alerts.sh

# Test specific alert
./scripts/test-alerts.sh high-error-rate
```

See `scripts/test-alerts.sh` for available test scenarios.

---

## 7. Alert Routing

Alerts are routed based on severity and category:

```
Critical Alerts → PagerDuty (if configured) + Slack (#alerts-critical)
Warning Alerts → Slack (#alerts-warnings)
Info Alerts → Email

Application Alerts → Slack (#alerts-application)
Infrastructure Alerts → Slack (#alerts-infrastructure)
Kafka Alerts → Slack (#alerts-kafka)
Dapr Alerts → Slack (#alerts-dapr)
```

---

## 8. Inhibition Rules

The following inhibition rules prevent alert storms:

1. **Critical inhibits Warning**: If a critical alert is firing, warning alerts for the same service are suppressed
2. **ServiceDown inhibits HighLatency**: If a service is down, latency alerts are suppressed
3. **PodRestartLoop inhibits Application**: If a pod is restarting, application alerts are suppressed
4. **DaprSidecarUnhealthy inhibits Dapr**: If Dapr sidecar is unhealthy, other Dapr alerts are suppressed

---

## 9. Silencing Alerts

### Temporary Silence (Maintenance Window)

```bash
# Silence all alerts for 2 hours
curl -X POST http://localhost:9093/api/v1/silences \
  -H "Content-Type: application/json" \
  -d '{
    "matchers": [{"name": "alertname", "value": ".*", "isRegex": true}],
    "startsAt": "2026-02-15T10:00:00Z",
    "endsAt": "2026-02-15T12:00:00Z",
    "createdBy": "admin",
    "comment": "Maintenance window"
  }'

# Silence specific service
curl -X POST http://localhost:9093/api/v1/silences \
  -H "Content-Type: application/json" \
  -d '{
    "matchers": [{"name": "service", "value": "backend"}],
    "startsAt": "2026-02-15T10:00:00Z",
    "endsAt": "2026-02-15T12:00:00Z",
    "createdBy": "admin",
    "comment": "Backend deployment"
  }'
```

### Via Alertmanager UI

1. Open Alertmanager UI: http://localhost:9093
2. Click "Silences" tab
3. Click "New Silence"
4. Configure matchers and duration
5. Click "Create"

---

## 10. Troubleshooting

### Alerts Not Firing

1. Check Prometheus is scraping metrics:
   ```bash
   kubectl logs -n monitoring -l app=prometheus
   ```

2. Verify alert rules are loaded:
   ```bash
   curl http://localhost:9090/api/v1/rules
   ```

3. Check alert evaluation:
   ```bash
   # Open Prometheus UI
   # Go to Alerts tab
   # Check if alerts are in "Pending" or "Firing" state
   ```

### Notifications Not Sent

1. Check Alertmanager logs:
   ```bash
   kubectl logs -n monitoring -l app=alertmanager
   ```

2. Verify Alertmanager configuration:
   ```bash
   curl http://localhost:9093/api/v1/status
   ```

3. Test notification channels:
   ```bash
   # Send test alert
   curl -X POST http://localhost:9093/api/v1/alerts \
     -H "Content-Type: application/json" \
     -d '[{
       "labels": {"alertname": "TestAlert", "severity": "warning"},
       "annotations": {"summary": "Test alert", "description": "This is a test"}
     }]'
   ```

### High Alert Volume

1. Review inhibition rules
2. Adjust alert thresholds
3. Increase group_interval and repeat_interval
4. Add more specific matchers to routes

---

## 11. Best Practices

1. **Alert on Symptoms, Not Causes**: Alert on user-facing issues (high latency, errors) rather than internal metrics
2. **Actionable Alerts**: Every alert should have a clear action to take
3. **Runbooks**: Link to runbooks in alert annotations
4. **Test Regularly**: Use test script to verify alerts work
5. **Review and Tune**: Regularly review alert thresholds and adjust based on actual behavior
6. **Avoid Alert Fatigue**: Too many alerts lead to ignored alerts
7. **Use Inhibition Rules**: Prevent alert storms with proper inhibition
8. **Document Escalation**: Clear escalation paths for different severity levels

---

## 12. Maintenance

### Updating Alert Rules

```bash
# Edit alert rules
vim monitoring/prometheus/alert-rules.yaml

# Apply changes
kubectl apply -f monitoring/prometheus/alert-rules.yaml

# Reload Prometheus configuration
kubectl exec -n monitoring prometheus-0 -- kill -HUP 1
```

### Updating Alertmanager Configuration

```bash
# Edit configuration
vim monitoring/alertmanager/alertmanager-config.yaml

# Apply changes
kubectl apply -f monitoring/alertmanager/alertmanager-config.yaml

# Reload Alertmanager
kubectl exec -n monitoring alertmanager-0 -- kill -HUP 1
```

### Backup and Restore

```bash
# Backup alert rules
kubectl get configmap prometheus-alert-rules -n monitoring -o yaml > alert-rules-backup.yaml

# Backup Alertmanager config
kubectl get configmap alertmanager-config -n monitoring -o yaml > alertmanager-config-backup.yaml

# Restore
kubectl apply -f alert-rules-backup.yaml
kubectl apply -f alertmanager-config-backup.yaml
```

---

## 13. References

- **Prometheus Alerting**: https://prometheus.io/docs/alerting/latest/overview/
- **Alertmanager Documentation**: https://prometheus.io/docs/alerting/latest/alertmanager/
- **Alert Rule Best Practices**: https://prometheus.io/docs/practices/alerting/
- **Notification Templates**: https://prometheus.io/docs/alerting/latest/notifications/

---

## 14. Support

For issues or questions:

1. Check Prometheus and Alertmanager logs
2. Review this documentation
3. Consult runbooks linked in alert annotations
4. Contact DevOps team: devops@todo-app.com

---

**END OF ALERTING SETUP GUIDE**

*Generated by cicd-monitoring-agent for Task T5.6.4*
