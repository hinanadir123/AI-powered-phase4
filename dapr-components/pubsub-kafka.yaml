# Dapr Pub/Sub Component: Kafka
# Task: T5.1.2 - Generate Dapr Component YAML Files
# Spec Reference: phase5-spec.md Section 4.3 (Dapr Components)
# Constitution: constitution.md v5.0 Section 4.3 (Dapr Components)
#
# This component enables Kafka-based publish-subscribe messaging via Dapr.
# It abstracts Kafka complexity and allows swapping to other Pub/Sub systems
# by changing only this YAML file (no code changes required).
#
# Version: 1.0
# Date: 2026-02-15
# Generated by: kafka-dapr-engineer agent

apiVersion: dapr.io/v1alpha1
kind: Component
metadata:
  name: pubsub-kafka
  namespace: default
spec:
  type: pubsub.kafka
  version: v1
  metadata:
    # Kafka Broker Configuration
    # Local development: localhost:19092 (Redpanda via docker-compose)
    # Production: Replace with actual broker addresses
    - name: brokers
      value: "localhost:19092"

    # Consumer Group Configuration
    # Backend API uses backend-group, Reminder Worker uses worker-group
    - name: consumerGroup
      value: "backend-group"

    # Client ID for identification in Kafka logs
    - name: clientID
      value: "dapr-todo-app"

    # Authentication Configuration (if using SASL)
    # Uncomment and configure if using Redpanda Cloud or Confluent Cloud
    # - name: authType
    #   value: "password"
    # - name: saslUsername
    #   secretKeyRef:
    #     name: kafka-secrets
    #     key: username
    # - name: saslPassword
    #   secretKeyRef:
    #     name: kafka-secrets
    #     key: password
    # - name: saslMechanism
    #   value: "SCRAM-SHA-256"

    # TLS Configuration (for production)
    # - name: enableTLS
    #   value: "true"
    # - name: skipVerify
    #   value: "false"

    # Consumer Configuration
    - name: consumeRetryInterval
      value: "200ms"

    - name: version
      value: "2.8.0"

    # Auto-offset reset strategy
    # earliest: Start from beginning if no offset exists
    # latest: Start from latest message if no offset exists
    - name: initialOffset
      value: "earliest"

    # Maximum number of messages to fetch in a single request
    - name: maxMessageBytes
      value: "1048576"  # 1MB

    # Session timeout for consumer group coordination
    - name: sessionTimeout
      value: "30s"

    # Heartbeat interval for consumer liveness
    - name: heartbeatInterval
      value: "10s"

    # Compression type for messages
    - name: compressionType
      value: "snappy"

    # Idempotent producer for exactly-once semantics
    - name: enableIdempotence
      value: "true"

    # Required acknowledgments for producer
    # all: Wait for all in-sync replicas to acknowledge
    - name: requiredAcks
      value: "all"

    # Producer retry configuration
    - name: maxRetries
      value: "3"

    - name: retryBackoff
      value: "100ms"

    # Dead Letter Queue Configuration
    # Failed messages are sent to {topic}-dlq
    - name: deadLetterTopic
      value: "task-events-dlq"

    # Maximum delivery attempts before sending to DLQ
    - name: maxDeliveryAttempts
      value: "5"

    # Exponential backoff for retries
    - name: retryBackoffMultiplier
      value: "2"

    - name: retryBackoffMaxInterval
      value: "30s"

# Scopes define which applications can use this component
# Only backend and worker services should publish/subscribe to Kafka
scopes:
  - todo-backend
  - todo-worker
  - reminder-worker

---
# Additional Pub/Sub Component for Worker Service
# This allows the worker to use a different consumer group

apiVersion: dapr.io/v1alpha1
kind: Component
metadata:
  name: pubsub-kafka-worker
  namespace: default
spec:
  type: pubsub.kafka
  version: v1
  metadata:
    - name: brokers
      value: "{KAFKA_BROKERS}"

    - name: consumerGroup
      value: "worker-group"

    - name: clientID
      value: "dapr-todo-worker"

    - name: consumeRetryInterval
      value: "200ms"

    - name: version
      value: "2.8.0"

    - name: initialOffset
      value: "earliest"

    - name: maxMessageBytes
      value: "1048576"

    - name: sessionTimeout
      value: "30s"

    - name: heartbeatInterval
      value: "10s"

    - name: compressionType
      value: "snappy"

    - name: enableIdempotence
      value: "true"

    - name: requiredAcks
      value: "all"

    - name: maxRetries
      value: "3"

    - name: retryBackoff
      value: "100ms"

    - name: deadLetterTopic
      value: "reminders-dlq"

    - name: maxDeliveryAttempts
      value: "5"

    - name: retryBackoffMultiplier
      value: "2"

    - name: retryBackoffMaxInterval
      value: "30s"

scopes:
  - todo-worker
  - reminder-worker

---
# Topic Subscription Configuration
# Define which topics each service subscribes to
#
# Usage in code:
# POST http://localhost:3500/v1.0/publish/pubsub-kafka/task-events
# Body: { "type": "task.created", "task_id": "123", "data": {...} }
#
# Subscription endpoint:
# GET http://localhost:3500/v1.0/subscribe
# Returns: [{ "pubsubname": "pubsub-kafka", "topic": "task-events", "route": "/events/task" }]
#
# Topics:
# - task-events: Task CRUD operations (created, updated, deleted, completed)
# - reminders: Reminder scheduling and notifications
# - task-updates: Real-time task synchronization for frontend
# - task-events-dlq: Dead letter queue for failed task events
# - reminders-dlq: Dead letter queue for failed reminders
# - task-updates-dlq: Dead letter queue for failed updates
